import streamlit as st
import pandas as pd
from pptx import Presentation
import os
import time
import requests
from bs4 import BeautifulSoup
import json
import re
import random
import shutil
import copy
import ast  # æ–°å¢ï¼šç”¨äºå¤„ç†å•å¼•å·æ ¼å¼çš„æ•°æ®
from io import BytesIO

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="äº¬ä¸œç›´æ’­æ‰‹å¡ç”Ÿæˆå™¨ Webç‰ˆ", page_icon="âš¡", layout="wide")

# --- æ ¸å¿ƒé€»è¾‘ ---
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://item.jd.com/",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Connection": "keep-alive"
    }

def scrape_jd_sku(sku):
    url = f"https://item.jd.com/{sku}.html"
    info = {"sku": sku, "title": "", "image_url": ""}
    
    try:
        r = requests.get(url, headers=get_headers(), timeout=15)
        if "verify" in r.url or "passport" in r.url:
            return None

        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, 'html.parser')
        
        raw_title = ""
        title_tag = soup.select_one("div.sku-name")
        if title_tag: raw_title = title_tag.get_text(strip=True)
        if not raw_title and soup.title: raw_title = soup.title.string.split('-')[0].strip()
        
        if raw_title:
            info["title"] = raw_title.replace("äº¬ä¸œ", "").replace("è‡ªè¥", "").strip()
        else:
            return None

        candidates = []
        img_tag = soup.select_one("#spec-img")
        if img_tag:
            candidates.append(img_tag.get('data-origin'))
            candidates.append(img_tag.get('src'))
        patterns = re.findall(r'//img\d{1,2}\.360buyimg\.com/n[01]/jfs/[^"]+\.jpg', r.text)
        candidates.extend(patterns)

        valid_imgs = []
        for img in candidates:
            if img and "jfs" in img and ".jpg" in img:
                if not img.startswith("http"):
                    img = "https:" + img if img.startswith("//") else "https://" + img
                img = img.replace("/n1/", "/n0/").replace("/n5/", "/n0/")
                valid_imgs.append(img)

        if valid_imgs:
            info["image_url"] = valid_imgs[0]
        
        return info
    except Exception as e:
        return None

def download_image(url, sku):
    if not url: return None
    try:
        r = requests.get(url, headers=get_headers(), timeout=15)
        return BytesIO(r.content)
    except:
        return None

def call_ai(product_name, api_key, base_url):
    if not api_key: return {}
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„ç”µå•†é‡‘ç‰Œé€‰å“æ€»ç›‘ã€‚è¯·æ ¹æ®å•†å“åç§°ã€{product_name}ã€‘ï¼Œæ’°å†™ 4 ä¸ªé«˜è½¬åŒ–ç›´æ’­å–ç‚¹ã€‚
    ã€è¦æ±‚ã€‘ï¼š
    1. ç»“æ„ï¼šã€å¸ç›çŸ­æ ‡é¢˜ã€‘+ã€ç—›ç‚¹åœºæ™¯/è§£å†³æ–¹æ¡ˆã€‘ã€‚
    2. è¯¦ç»†å…·ä½“ï¼šæ‹’ç»ç©ºè¯ï¼Œè¦æœ‰ç”»é¢æ„Ÿã€‚
    3. æ•°é‡ï¼šå¿…é¡» 4 æ¡ã€‚
    ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
    å¿…é¡»è¿”å›çº¯ JSON æ ¼å¼ï¼Œä¸è¦åŠ  ```json ç­‰æ ‡è®°ï¼Œé”®åä¸ºï¼šselling_point_1, selling_point_2, selling_point_3, selling_point_4ã€‚
    """
    
    data = {
        "model": "deepseek-chat", 
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.8,
        "response_format": {"type": "json_object"}
    }
    try:
        resp = requests.post(f"{base_url}/chat/completions", headers=headers, json=data, timeout=40)
        content = resp.json()['choices'][0]['message']['content']
        
        # --- V4.1 æ ¸å¿ƒä¿®å¤ï¼šå¼ºåŠ›æ¸…æ´—æ•°æ® ---
        # 1. å»æ‰ Markdown ä»£ç å—ç¬¦å·
        content = content.replace("```json", "").replace("```", "").strip()
        
        # 2. å°è¯•æ ‡å‡† JSON è§£æ
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            # 3. å¦‚æœå¤±è´¥ï¼ˆæ¯”å¦‚å› ä¸ºå•å¼•å·ï¼‰ï¼Œå°è¯•ç”¨ Python è¯­æ³•è§£æ
            try:
                return ast.literal_eval(content)
            except:
                # 4. å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸ï¼Œé˜²æ­¢æŠ¥é”™
                return {}
    except:
        return {}

def duplicate_slide(pres):
    source = pres.slides[0]
    blank_slide_layout = pres.slide_layouts[6] 
    dest = pres.slides.add_slide(blank_slide_layout)

    for shp in source.shapes:
        el = shp.element
        newel = copy.deepcopy(el)
        dest.shapes._spTree.insert_element_before(newel, 'p:extLst')
    return dest

def fill_slide(slide, data):
    def replace(name, text):
        for shape in slide.shapes:
            if shape.name == name and shape.has_text_frame:
                shape.text_frame.text = str(text)
                return
            if shape.shape_type == 6: # Group
                for sub in shape.shapes:
                    if sub.name == name and sub.has_text_frame:
                        sub.text_frame.text = str(text)
                        return

    replace("product_name", data['title'])
    replace("product_sku", data['sku']) 
    replace("price_live", data['price'])
    
    points = data.get('points', {})
    # å¦‚æœ points æ˜¯ç©ºçš„ï¼ˆè§£æå¤±è´¥ï¼‰ï¼Œå¡«å…¥é»˜è®¤æç¤º
    if not points:
        for i in range(1, 5):
            replace(f"selling_point_{i}", "AI ç”Ÿæˆè¶…æ—¶æˆ–æ ¼å¼é”™è¯¯ï¼Œè¯·æ‰‹åŠ¨å¡«å†™")
    else:
        for i in range(1, 5):
            content = points.get(f'selling_point_{i}', '')
            content = re.sub(r'^\d+\.?\s*', '', str(content))
            replace(f"selling_point_{i}", content)

    if data['image_data']:
        for shape in slide.shapes:
            if shape.name == "product_image":
                left, top, width, height = shape.left, shape.top, shape.width, shape.height
                sp = shape._element
                sp.getparent().remove(sp)
                slide.shapes.add_picture(data['image_data'], left, top, width, height)
                break

# --- ç½‘é¡µç•Œé¢ ---
st.title("âš¡ äº¬ä¸œç›´æ’­æ‰‹å¡å…¨è‡ªåŠ¨ç”Ÿæˆå™¨ (V4.1 ä¿®å¤ä¹±ç ç‰ˆ)")
st.markdown("å‡çº§è¯´æ˜ï¼šä¿®å¤äº†å–ç‚¹ç”Ÿæˆå˜æˆä»£ç ä¹±ç çš„é—®é¢˜ï¼")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    api_key = st.text_input("AI API Key", type="password", help="è¾“å…¥DeepSeek Key")
    base_url = st.text_input("Base URL", value="[https://api.deepseek.com](https://api.deepseek.com)")
    
    st.markdown("---")
    st.info("ğŸ’¡ è¯·ç¡®ä¿ã€ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptxã€‘å·²ä¸Šä¼ åˆ°æœåŠ¡å™¨ç›®å½•")
    
    uploaded_template = st.file_uploader("æˆ–ä¸Šä¼ ä½ çš„PPTæ¨¡æ¿", type="pptx")
    if uploaded_template:
        with open("ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx", "wb") as f:
            f.write(uploaded_template.getbuffer())
        st.success("æ¨¡æ¿å·²æ›´æ–°ï¼")

# ä¸»ç•Œé¢
col1, col2 = st.columns([1, 1])
with col1:
    skus_input = st.text_area("1. è¾“å…¥ SKU (æ‰¹é‡ï¼Œé€—å·æˆ–æ¢è¡Œåˆ†éš”)", height=200, placeholder="1000123456\n1000888888")
with col2:
    prices_input = st.text_area("2. è¾“å…¥ç›´æ’­ä¸“äº«ä»· (å¯¹åº”å·¦ä¾§SKUé¡ºåº)", height=200, placeholder="9.9\n12.8\n(å¦‚æœåªå¡«ä¸€ä¸ªï¼Œåˆ™å…¨éƒ¨é€šç”¨)")
    st.caption("æ³¨ï¼šç¬¬ä¸€è¡Œä»·æ ¼å¯¹åº”ç¬¬ä¸€è¡ŒSKUï¼Œä»¥æ­¤ç±»æ¨ã€‚")

if st.button("ğŸš€ å¼€å§‹ç”Ÿæˆåˆé›†", type="primary"):
    if not skus_input:
        st.error("è¯·è¾“å…¥ SKU")
        st.stop()
        
    if not os.path.exists("ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx"):
        st.error("æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶ï¼è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ¨¡æ¿ã€‚")
        st.stop()
    
    prs = Presentation("ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx")
    
    skus_text = skus_input.replace('ï¼Œ', ',').replace('\n', ',').replace(' ', ',')
    skus = [s.strip() for s in skus_text.split(',') if s.strip()]
    
    prices_text = prices_input.replace('ï¼Œ', ',').replace('\n', ',').replace(' ', ',')
    prices = [p.strip() for p in prices_text.split(',') if p.strip()]
    if not prices: prices = ["9.9"]
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    success_count = 0
    
    for i, sku in enumerate(skus):
        if i > 0:
            sleep_time = random.uniform(2, 5)
            status_text.text(f"â³ é˜²å°æš‚åœ {int(sleep_time)} ç§’...")
            time.sleep(sleep_time)
            
        status_text.text(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(skus)} ä¸ªå•†å“: {sku} ...")
        
        current_price = prices[i] if i < len(prices) else prices[-1]
        
        info = scrape_jd_sku(sku)
        if not info:
            st.warning(f"SKU {sku} æŠ“å–å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
            continue
            
        info['price'] = current_price
        info['image_data'] = download_image(info['image_url'], sku)
        
        if api_key:
            info['points'] = call_ai(info['title'], api_key, base_url)
        else:
            info['points'] = {}
            
        if i == 0:
            current_slide = prs.slides[0]
        else:
            current_slide = duplicate_slide(prs)
            
        fill_slide(current_slide, info)
        success_count += 1
        
        progress_bar.progress((i + 1) / len(skus))
    
    status_text.text("æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
    
    output_ppt = BytesIO()
    prs.save(output_ppt)
    output_ppt.seek(0)
    
    if success_count > 0:
        st.success(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {success_count} å¼ æ‰‹å¡ï¼")
        st.download_button(
            label="â¬‡ï¸ ä¸‹è½½ç›´æ’­æ‰‹å¡åˆé›† (PPTX)",
            data=output_ppt,
            file_name=f"ç›´æ’­æ‰‹å¡åˆé›†_{time.strftime('%m%d_%H%M')}.pptx",
            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation"
        )
    else:
        st.error("æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆæ‰‹å¡ï¼Œè¯·æ£€æŸ¥ SKU æˆ–ç½‘ç»œã€‚")