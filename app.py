import streamlit as st
import pandas as pd
from pptx import Presentation
import os
import time
import requests
from bs4 import BeautifulSoup
import json
import re
import random
import shutil
import zipfile
from io import BytesIO

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="äº¬ä¸œç›´æ’­æ‰‹å¡ç”Ÿæˆå™¨ Webç‰ˆ", page_icon="âš¡", layout="wide")

# --- æ ¸å¿ƒé€»è¾‘ (å¤ç”¨ V2.7) ---
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "[https://item.jd.com/](https://item.jd.com/)",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }

def scrape_jd_sku(sku):
    url = f"[https://item.jd.com/](https://item.jd.com/){sku}.html"
    info = {"sku": sku, "title": "", "image_url": ""}
    
    try:
        r = requests.get(url, headers=get_headers(), timeout=10)
        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, 'html.parser')
        
        # æŠ“æ ‡é¢˜
        raw_title = ""
        title_tag = soup.select_one("div.sku-name")
        if title_tag: raw_title = title_tag.get_text(strip=True)
        if not raw_title and soup.title: raw_title = soup.title.string.split('-')[0].strip()
        
        if raw_title:
            info["title"] = raw_title.replace("äº¬ä¸œ", "").replace("è‡ªè¥", "").strip()
        else:
            info["title"] = f"å•†å“_{sku}"

        # æŠ“ä¸»å›¾
        candidates = []
        img_tag = soup.select_one("#spec-img")
        if img_tag:
            candidates.append(img_tag.get('data-origin'))
            candidates.append(img_tag.get('src'))
        patterns = re.findall(r'//img\d{1,2}\.360buyimg\.com/n[01]/jfs/[^"]+\.jpg', r.text)
        candidates.extend(patterns)

        valid_imgs = []
        for img in candidates:
            if img and "jfs" in img and ".jpg" in img:
                if not img.startswith("http"):
                    img = "https:" + img if img.startswith("//") else "https://" + img
                img = img.replace("/n1/", "/n0/").replace("/n5/", "/n0/")
                valid_imgs.append(img)

        if valid_imgs:
            info["image_url"] = valid_imgs[0]
        
        return info
    except Exception as e:
        return None

def download_image(url, sku):
    if not url: return None
    try:
        r = requests.get(url, headers=get_headers(), timeout=10)
        filename = f"temp_img_{sku}.jpg"
        with open(filename, 'wb') as f: f.write(r.content)
        return filename
    except:
        return None

def call_ai(product_name, api_key, base_url):
    if not api_key: return {}
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    prompt = f"""
    ä½ æ˜¯ä¸€åå¸¦è´§ä¸»æ’­ã€‚è¯·æ ¹æ®å•†å“åã€{product_name}ã€‘ï¼Œæç‚¼ 4 ä¸ªé€‚åˆå£æ’­çš„â€œé«˜è½¬åŒ–å–ç‚¹â€ã€‚
    è¦æ±‚ï¼šå£è¯­åŒ–ï¼Œç»“æ„ä¸º[åœºæ™¯]+[åˆ©ç›Šç‚¹]ã€‚éœ€ç”Ÿæˆ4æ¡ã€‚
    è¾“å‡ºæ ¼å¼ï¼šè¿”å› JSONï¼Œkey ä¸º selling_point_1 åˆ° selling_point_4ã€‚
    """
    data = {
        "model": "deepseek-chat", 
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.8,
        "response_format": {"type": "json_object"}
    }
    try:
        resp = requests.post(f"{base_url}/chat/completions", headers=headers, json=data, timeout=30)
        return json.loads(resp.json()['choices'][0]['message']['content'])
    except:
        return {}

def generate_ppt(data, template_path, output_dir):
    if not os.path.exists(template_path): return None
    sku = data['sku']
    prs = Presentation(template_path)
    slide = prs.slides[0]

    def replace(name, text):
        for shape in slide.shapes:
            if shape.name == name and shape.has_text_frame:
                shape.text_frame.text = str(text)
                return
            if shape.shape_type == 6: 
                for sub in shape.shapes:
                    if sub.name == name and sub.has_text_frame:
                        sub.text_frame.text = str(text)
                        return

    replace("product_name", data['title'])
    replace("product_sku", data['sku']) 
    replace("price_live", data['price'])
    
    points = data.get('points', {})
    for i in range(1, 5):
        replace(f"selling_point_{i}", points.get(f'selling_point_{i}', ''))

    if data['image_local']:
        for shape in slide.shapes:
            if shape.name == "product_image":
                left, top, width, height = shape.left, shape.top, shape.width, shape.height
                sp = shape._element
                sp.getparent().remove(sp)
                slide.shapes.add_picture(data['image_local'], left, top, width, height)
                break
    
    save_path = os.path.join(output_dir, f"{sku}.pptx")
    prs.save(save_path)
    return save_path

# --- ç½‘é¡µç•Œé¢ ---
st.title("âš¡ äº¬ä¸œç›´æ’­æ‰‹å¡å…¨è‡ªåŠ¨ç”Ÿæˆå™¨ (Webç‰ˆ)")
st.markdown("ä¸ç”¨å®‰è£…è½¯ä»¶ï¼Œè¾“å…¥SKUç›´æ¥ä¸‹è½½PPTæºæ–‡ä»¶ï¼")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    api_key = st.text_input("AI API Key", type="password", help="è¾“å…¥DeepSeek Key")
    base_url = st.text_input("Base URL", value="[https://api.deepseek.com](https://api.deepseek.com)")
    
    st.markdown("---")
    st.info("ğŸ’¡ è¯·ç¡®ä¿ã€ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptxã€‘å·²ä¸Šä¼ åˆ°æœåŠ¡å™¨ç›®å½•")
    
    # å…è®¸ç”¨æˆ·ä¸Šä¼ æ¨¡æ¿
    uploaded_template = st.file_uploader("æˆ–ä¸Šä¼ ä½ çš„PPTæ¨¡æ¿", type="pptx")
    if uploaded_template:
        with open("ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx", "wb") as f:
            f.write(uploaded_template.getbuffer())
        st.success("æ¨¡æ¿å·²æ›´æ–°ï¼")

# ä¸»ç•Œé¢ï¼šåˆ†ä¸ºå·¦å³ä¸¤åˆ—ï¼Œå·¦è¾¹å¡«SKUï¼Œå³è¾¹å¡«ä»·æ ¼
col1, col2 = st.columns([1, 1])
with col1:
    skus_input = st.text_area("1. è¾“å…¥ SKU (æ‰¹é‡ï¼Œé€—å·æˆ–æ¢è¡Œåˆ†éš”)", height=200, placeholder="1000123456\n1000888888")
with col2:
    prices_input = st.text_area("2. è¾“å…¥ç›´æ’­ä¸“äº«ä»· (å¯¹åº”å·¦ä¾§SKUé¡ºåº)", height=200, placeholder="9.9\n12.8\n(å¦‚æœåªå¡«ä¸€ä¸ªï¼Œåˆ™å…¨éƒ¨é€šç”¨)")
    st.caption("æ³¨ï¼šç¬¬ä¸€è¡Œä»·æ ¼å¯¹åº”ç¬¬ä¸€è¡ŒSKUï¼Œä»¥æ­¤ç±»æ¨ã€‚å¦‚æœä»·æ ¼è¾“å°‘äº†ï¼Œå‰©ä¸‹çš„å•†å“ä¼šè‡ªåŠ¨å¤ç”¨æœ€åä¸€ä¸ªä»·æ ¼ã€‚")

if st.button("ğŸš€ å¼€å§‹ç”Ÿæˆ", type="primary"):
    if not skus_input:
        st.error("è¯·è¾“å…¥ SKU")
        st.stop()
    
    # å‡†å¤‡ç¯å¢ƒ
    output_dir = "web_output"
    if os.path.exists(output_dir): shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    
    # è§£æSKU
    skus_text = skus_input.replace('ï¼Œ', ',').replace('\n', ',').replace(' ', ',')
    skus = [s.strip() for s in skus_text.split(',') if s.strip()]
    
    # è§£æä»·æ ¼
    prices_text = prices_input.replace('ï¼Œ', ',').replace('\n', ',').replace(' ', ',')
    prices = [p.strip() for p in prices_text.split(',') if p.strip()]
    if not prices: prices = ["9.9"] # å…œåº•é»˜è®¤å€¼
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    generated_files = []
    
    for i, sku in enumerate(skus):
        status_text.text(f"æ­£åœ¨å¤„ç†: {sku} ({i+1}/{len(skus)})...")
        
        # è·å–å¯¹åº”ä»·æ ¼ï¼šå¦‚æœiåœ¨ä»·æ ¼åˆ—è¡¨èŒƒå›´å†…ï¼Œå–å¯¹åº”å€¼ï¼›å¦åˆ™å–æœ€åä¸€ä¸ª
        if i < len(prices):
            current_price = prices[i]
        else:
            current_price = prices[-1] 
        
        # 1. æŠ“å–
        info = scrape_jd_sku(sku)
        if not info:
            st.warning(f"SKU {sku} æŠ“å–å¤±è´¥ï¼Œè·³è¿‡")
            continue
            
        info['price'] = current_price
        
        # 2. å›¾ç‰‡
        info['image_local'] = download_image(info['image_url'], sku)
        
        # 3. AI
        if api_key:
            info['points'] = call_ai(info['title'], api_key, base_url)
        else:
            info['points'] = {}
        
        # 4. ç”ŸæˆPPT
        ppt_path = generate_ppt(info, "ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx", output_dir)
        if ppt_path:
            generated_files.append(ppt_path)
        
        # æ¸…ç†å›¾ç‰‡
        if info['image_local'] and os.path.exists(info['image_local']):
            os.remove(info['image_local'])
            
        progress_bar.progress((i + 1) / len(skus))
    
    status_text.text("å¤„ç†å®Œæˆï¼æ­£åœ¨æ‰“åŒ…...")
    
    # æ‰“åŒ…ä¸‹è½½
    if generated_files:
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zf:
            for file_path in generated_files:
                zf.write(file_path, os.path.basename(file_path))
        
        st.success(f"æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶ï¼")
        st.download_button(
            label="â¬‡ï¸ ä¸‹è½½æ‰€æœ‰æ‰‹å¡ (ZIP)",
            data=zip_buffer.getvalue(),
            file_name="ç›´æ’­æ‰‹å¡åˆé›†.zip",
            mime="application/zip"
        )
    else:
        st.error("æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ SKU æˆ– æ¨¡æ¿ã€‚")

