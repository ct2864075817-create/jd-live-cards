import streamlit as st
import pandas as pd
from pptx import Presentation
import os
import time
import requests
from bs4 import BeautifulSoup
import json
import re
import random
import shutil
import copy
import ast
import zipfile
from io import BytesIO

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="äº¬ä¸œç›´æ’­æ‰‹å¡ç”Ÿæˆå™¨ (V3.0 ZIPç‰ˆ)", page_icon="âš¡", layout="wide")

# --- æ ¸å¿ƒé€»è¾‘ ---
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "[https://item.jd.com/](https://item.jd.com/)",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Connection": "keep-alive"
    }

def scrape_jd_sku(sku):
    # ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨çº¯æ–‡æœ¬ç½‘å€
    url = f"[https://item.jd.com/](https://item.jd.com/){sku}.html"
    info = {"sku": sku, "title": "", "image_url": ""}
    
    try:
        r = requests.get(url, headers=get_headers(), timeout=15)
        if "verify" in r.url or "passport" in r.url:
            return None

        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, 'html.parser')
        
        raw_title = ""
        title_tag = soup.select_one("div.sku-name")
        if title_tag: raw_title = title_tag.get_text(strip=True)
        if not raw_title and soup.title: raw_title = soup.title.string.split('-')[0].strip()
        
        if raw_title:
            info["title"] = raw_title.replace("äº¬ä¸œ", "").replace("è‡ªè¥", "").strip()
        else:
            return None

        candidates = []
        img_tag = soup.select_one("#spec-img")
        if img_tag:
            candidates.append(img_tag.get('data-origin'))
            candidates.append(img_tag.get('src'))
        patterns = re.findall(r'//img\d{1,2}\.360buyimg\.com/n[01]/jfs/[^"]+\.jpg', r.text)
        candidates.extend(patterns)

        valid_imgs = []
        for img in candidates:
            if img and "jfs" in img and ".jpg" in img:
                if not img.startswith("http"):
                    img = "https:" + img if img.startswith("//") else "https://" + img
                img = img.replace("/n1/", "/n0/").replace("/n5/", "/n0/")
                valid_imgs.append(img)

        if valid_imgs:
            info["image_url"] = valid_imgs[0]
        
        return info
    except Exception as e:
        return None

def download_image(url, sku):
    if not url: return None
    try:
        r = requests.get(url, headers=get_headers(), timeout=15)
        return BytesIO(r.content)
    except:
        return None

def extract_points_with_regex(text):
    points = {}
    for i in range(1, 5):
        key = f"selling_point_{i}"
        pattern = re.search(rf"['\"]?{key}['\"]?\s*:\s*['\"](.*?)['\"]", text, re.DOTALL)
        if pattern:
            points[key] = pattern.group(1)
    return points

def call_ai(product_name, api_key, base_url):
    if not api_key: return {}
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„ç”µå•†é‡‘ç‰Œé€‰å“æ€»ç›‘ï¼Œæ“…é•¿æŒ–æ˜â€œç—›ç‚¹è¥é”€â€å’Œâ€œé«˜è½¬åŒ–è¯æœ¯â€ã€‚
    è¯·æ ¹æ®å•†å“åç§°ã€{product_name}ã€‘ï¼Œæ·±åº¦å‰–æç”¨æˆ·ç—›ç‚¹ï¼Œæ’°å†™ 4 ä¸ªæå…·ç…½åŠ¨æ€§å’Œè½¬åŒ–åŠ›çš„ç›´æ’­æ‰‹å¡å–ç‚¹ã€‚

    ã€æ ¸å¿ƒè¦æ±‚ã€‘ï¼š
    1. **æ‹’ç»ç©ºè¯**ï¼šä¸è¦åªè¯´â€œå¥½ç”¨â€ã€â€œä¾¿å®œâ€ï¼Œè¦è¯´å‡ºå…·ä½“å¥½åœ¨å“ªé‡Œï¼Œè§£å†³ä»€ä¹ˆå…·ä½“éº»çƒ¦ã€‚
    2. **ç»“æ„ä¸¥æ ¼**ï¼šé‡‡ç”¨â€œç—›ç‚¹åœºæ™¯ + è§£å†³æ–¹æ¡ˆ + å¸¦æ¥çš„åˆ©ç›Šâ€çš„ç»“æ„ã€‚
    3. **è¯¦ç»†å…·ä½“**ï¼šæ¯æ¡å–ç‚¹éœ€åŒ…å«ä¸€ä¸ªã€å¸ç›çŸ­æ ‡é¢˜ã€‘ï¼ˆ6-10å­—ï¼‰å’Œä¸€æ®µã€è¯¦ç»†ç—›ç‚¹é˜è¿°ã€‘ï¼ˆ30-50å­—ï¼‰ã€‚
    4. **æ•°é‡**ï¼šå¿…é¡»ç”Ÿæˆ 4 æ¡ã€‚

    ã€å‚è€ƒèŒƒä¾‹ï¼ˆä»¥ä¿æ¸©æ¯ä¸ºä¾‹ï¼‰ã€‘ï¼š
    - å–ç‚¹1ï¼š**æ‹’ç»å–å†·æ°´ï¼Œ24å°æ—¶é”æ¸©**ï¼šä¸Šç­å¿™èµ·æ¥æ€»å¿˜å–æ°´ï¼Œæƒ³å–æ—¶æ°´æ—©å‡‰äº†ä¼¤èƒƒï¼Ÿå®ƒé‡‡ç”¨åŒå±‚æŠ½çœŸç©ºæŠ€æœ¯ï¼Œæ—©ä¸Šå€’çš„çƒ­æ°´ï¼Œæ™šä¸Šè¿˜æ˜¯çƒ«å˜´çš„ï¼Œéšæ—¶æ¸©æš–ä½ çš„èƒƒã€‚
    - å–ç‚¹2ï¼š**ä¸æ¼æ°´æ‰æ˜¯ç¡¬é“ç†**ï¼šåŒ…é‡Œæ–‡ä»¶ç”µè„‘æœ€æ€•æ°´æ¯æ¼æ°´ï¼è¿™æ¬¾é‡‡ç”¨é£Ÿå“çº§ç¡…èƒ¶å¯†å°åœˆï¼Œå€’ç½®ç‹‚ç”©éƒ½ä¸æ¼ï¼Œæ”¾å¿ƒéšä¾¿å¡è¿›åŒ…é‡Œï¼Œå‡ºè¡Œæ›´å®‰å¿ƒã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
    è¯·ç›´æ¥è¿”å›çº¯ JSON æ ¼å¼æ•°æ®ï¼Œé”®åå›ºå®šä¸ºï¼šselling_point_1, selling_point_2, selling_point_3, selling_point_4ã€‚
    """
    
    data = {
        "model": "deepseek-chat", 
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.8,
        "response_format": {"type": "json_object"}
    }
    try:
        # ä¿®æ­£ï¼šç¡®ä¿ Base URL ä¹Ÿæ˜¯çº¯å‡€çš„
        clean_base_url = base_url.strip().rstrip('/')
        if not clean_base_url.startswith('http'): 
            clean_base_url = "[https://api.deepseek.com](https://api.deepseek.com)"
        
        resp = requests.post(f"{clean_base_url}/chat/completions", headers=headers, json=data, timeout=40)
        content = resp.json()['choices'][0]['message']['content']
        content = content.replace("```json", "").replace("```", "").strip()
        
        try:
            return json.loads(content)
        except:
            try:
                return ast.literal_eval(content)
            except:
                return extract_points_with_regex(content)
    except:
        return {}

def generate_ppt(data, template_path, output_dir):
    if not os.path.exists(template_path): return None
    sku = data['sku']
    prs = Presentation(template_path)
    slide = prs.slides[0]

    def replace(name, text):
        text_str = str(text)
        if text_str.strip().startswith("{") and "selling_point" in text_str:
             text_str = "AIç”Ÿæˆæ ¼å¼é”™è¯¯ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹"
        for shape in slide.shapes:
            if shape.name == name and shape.has_text_frame:
                shape.text_frame.text = text_str
                return
            if shape.shape_type == 6: 
                for sub in shape.shapes:
                    if sub.name == name and sub.has_text_frame:
                        sub.text_frame.text = text_str
                        return

    replace("product_name", data['title'])
    replace("product_sku", data['sku']) 
    replace("price_live", data['price'])
    
    points = data.get('points', {})
    if not points:
        for i in range(1, 5):
            replace(f"selling_point_{i}", "æ™ºèƒ½å–ç‚¹ç”Ÿæˆå¤±è´¥")
    else:
        for i in range(1, 5):
            content = points.get(f'selling_point_{i}', '')
            content = re.sub(r'^\d+\.?\s*', '', str(content))
            replace(f"selling_point_{i}", content)

    if data['image_data']:
        for shape in slide.shapes:
            if shape.name == "product_image":
                left, top, width, height = shape.left, shape.top, shape.width, shape.height
                sp = shape._element
                sp.getparent().remove(sp)
                slide.shapes.add_picture(data['image_data'], left, top, width, height)
                break
    
    # ä¿å­˜ä¸ºå•ç‹¬çš„PPTæ–‡ä»¶
    save_path = os.path.join(output_dir, f"{sku}.pptx")
    prs.save(save_path)
    return save_path

# --- ç½‘é¡µç•Œé¢ ---
st.title("âš¡ äº¬ä¸œç›´æ’­æ‰‹å¡ç”Ÿæˆå™¨ (V3.0 ZIPç‰ˆ)")
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    api_key = st.text_input("AI API Key", type="password", help="è¾“å…¥DeepSeek Key")
    # ä¿®æ­£ï¼šé»˜è®¤å€¼å»æ‰äº† Markdown æ ¼å¼
    base_url = st.text_input("Base URL", value="[https://api.deepseek.com](https://api.deepseek.com)")
    uploaded_template = st.file_uploader("æˆ–ä¸Šä¼ ä½ çš„PPTæ¨¡æ¿", type="pptx")
    if uploaded_template:
        with open("ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx", "wb") as f:
            f.write(uploaded_template.getbuffer())
        st.success("æ¨¡æ¿å·²æ›´æ–°ï¼")

col1, col2 = st.columns([1, 1])
with col1:
    skus_input = st.text_area("1. è¾“å…¥ SKU (æ‰¹é‡)", height=200, placeholder="1000123456\n1000888888")
with col2:
    prices_input = st.text_area("2. è¾“å…¥ç›´æ’­ä¸“äº«ä»·", height=200, placeholder="9.9\n12.8")

if st.button("ğŸš€ å¼€å§‹ç”Ÿæˆ (ZIPæ‰“åŒ…)", type="primary"):
    if not skus_input:
        st.error("è¯·è¾“å…¥ SKU")
        st.stop()
    if not os.path.exists("ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx"):
        st.error("æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶ï¼")
        st.stop()
    
    # å‡†å¤‡ä¸´æ—¶ç›®å½•
    output_dir = "temp_output_cards"
    if os.path.exists(output_dir): shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    
    skus_text = skus_input.replace('ï¼Œ', ',').replace('\n', ',').replace(' ', ',')
    skus = [s.strip() for s in skus_text.split(',') if s.strip()]
    prices_text = prices_input.replace('ï¼Œ', ',').replace('\n', ',').replace(' ', ',')
    prices = [p.strip() for p in prices_text.split(',') if p.strip()]
    if not prices: prices = ["9.9"]
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    generated_files = []
    
    for i, sku in enumerate(skus):
        if i > 0:
            sleep_time = random.uniform(2, 5)
            status_text.text(f"â³ é˜²å°æš‚åœ {int(sleep_time)} ç§’...")
            time.sleep(sleep_time)
        status_text.text(f"å¤„ç†ä¸­: {sku}...")
        
        current_price = prices[i] if i < len(prices) else prices[-1]
        info = scrape_jd_sku(sku)
        if not info:
            st.warning(f"SKU {sku} æŠ“å–å¤±è´¥")
            continue
            
        info['price'] = current_price
        info['image_data'] = download_image(info['image_url'], sku)
        if api_key:
            info['points'] = call_ai(info['title'], api_key, base_url)
        else:
            info['points'] = {}
            
        # V3.0 é€»è¾‘ï¼šç”Ÿæˆç‹¬ç«‹æ–‡ä»¶
        ppt_path = generate_ppt(info, "ç›´æ’­æ‰‹å¡æ¨¡æ¿.pptx", output_dir)
        if ppt_path:
            generated_files.append(ppt_path)
        
        progress_bar.progress((i + 1) / len(skus))
    
    # æ‰“åŒ… ZIP
    if generated_files:
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zf:
            for file_path in generated_files:
                zf.write(file_path, os.path.basename(file_path))
        
        st.success(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶ï¼")
        st.download_button(
            label="â¬‡ï¸ ä¸‹è½½ ZIP å‹ç¼©åŒ…",
            data=zip_buffer.getvalue(),
            file_name="ç›´æ’­æ‰‹å¡åˆé›†.zip",
            mime="application/zip"
        )
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(output_dir)
    else:
        st.error("ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥SKU")

